{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This article will explore a huge variety of recurrent neural network architectures. This article is mainly motivated by this paper \"An Empirical Exploration of Recurrent Network Architectures\" and we will implement the network architecures that were highlighted in the paper. \n",
    "\n",
    "The implementation will be using Google's Tensorflow and we will implement the code on top of the Recurrent Neural Network example in the tensorflow website and on top of the code written by Jim Flemming in this blog article here: https://github.com/fomorians/lstm-odyssey. I have updated the code based on the new tensorflow version (0.12 as of today) and the recurrent neural network architectures are based on the paper \"An Empirical Exploration of Recurrent Network Architectures\" http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.nn.rnn_cell import GRUCell\n",
    "\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "\n",
    "sequence_loss_by_example = tf.nn.seq2seq.sequence_loss_by_example\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "# import variants\n",
    "from variants.lstm import VanillaLSTMCell\n",
    "from variants.vanillaRNN import VanillaRNNCell\n",
    "from variants.gru import GruCell\n",
    "from variants.mut1 import MUT1\n",
    "from variants.mut2 import MUT2\n",
    "# from variants.nig import NIGLSTMCell\n",
    "# from variants.nfg import NFGLSTMCell\n",
    "# from variants.nog import NOGLSTMCell\n",
    "# from variants.niaf import NIAFLSTMCell\n",
    "# from variants.noaf import NOAFLSTMCell\n",
    "# from variants.np import NPLSTMCell\n",
    "# from variants.cifg import CIFGLSTMCell\n",
    "# from variants.fgr import FGRLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../../../Machine175/AttentionModels/rnn/data/simple-examples/data/'\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './mut1_model/'\n",
    "checkpoint_path = './mut1_model'\n",
    "summary_path = './mut1_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "\n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # initializer used for reusable variable initializer (see `get_variable`)\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "\n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "\n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "\n",
    "        self.final_state = states[-1]\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                                    [size, vocab_size],\n",
    "                                    initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape our target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "\n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "\n",
    "        if is_training:\n",
    "            # setup learning rate variable to decay\n",
    "            self.lr = tf.Variable(1.0, trainable=False)\n",
    "\n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning Rate: 1.000\n",
      "0.0% Perplexity: 10131.736 (Cost: 322.820) Speed: 518 wps\n",
      "0.8% Perplexity: 1529349.860 (Cost: 497.102) Speed: 3101 wps\n",
      "1.5% Perplexity: 239853.457 (Cost: 328.559) Speed: 4201 wps\n",
      "2.3% Perplexity: 249465.778 (Cost: 402.614) Speed: 4800 wps\n",
      "3.0% Perplexity: 209693.106 (Cost: 361.256) Speed: 5173 wps\n",
      "3.8% Perplexity: 185490.161 (Cost: 496.474) Speed: 5443 wps\n",
      "4.5% Perplexity: 140047.142 (Cost: 398.940) Speed: 5630 wps\n",
      "5.3% Perplexity: 134696.243 (Cost: 299.854) Speed: 5774 wps\n",
      "6.0% Perplexity: 98279.215 (Cost: 334.812) Speed: 5892 wps\n",
      "6.8% Perplexity: 75665.107 (Cost: 290.210) Speed: 5986 wps\n",
      "7.5% Perplexity: 63530.657 (Cost: 301.823) Speed: 6062 wps\n",
      "8.3% Perplexity: 53190.502 (Cost: 310.727) Speed: 6122 wps\n",
      "9.0% Perplexity: 44882.543 (Cost: 267.843) Speed: 6177 wps\n",
      "9.8% Perplexity: 38674.859 (Cost: 289.231) Speed: 6224 wps\n",
      "10.6% Perplexity: 35039.363 (Cost: 283.897) Speed: 6267 wps\n",
      "11.3% Perplexity: 31081.637 (Cost: 275.331) Speed: 6303 wps\n",
      "12.1% Perplexity: 29659.943 (Cost: 269.563) Speed: 6335 wps\n",
      "12.8% Perplexity: 28975.436 (Cost: 346.107) Speed: 6359 wps\n",
      "13.6% Perplexity: 28541.966 (Cost: 295.850) Speed: 6387 wps\n",
      "14.3% Perplexity: 28410.215 (Cost: 301.668) Speed: 6409 wps\n",
      "15.1% Perplexity: 26630.872 (Cost: 268.348) Speed: 6430 wps\n",
      "15.8% Perplexity: 25976.937 (Cost: 293.379) Speed: 6451 wps\n",
      "16.6% Perplexity: 25772.796 (Cost: 572.802) Speed: 6467 wps\n",
      "17.3% Perplexity: 26166.216 (Cost: 457.184) Speed: 6484 wps\n",
      "18.1% Perplexity: 31645.547 (Cost: 614.010) Speed: 6499 wps\n",
      "18.8% Perplexity: 34138.382 (Cost: 285.797) Speed: 6514 wps\n",
      "19.6% Perplexity: 33991.433 (Cost: 287.223) Speed: 6527 wps\n",
      "20.3% Perplexity: 34967.738 (Cost: 341.214) Speed: 6537 wps\n",
      "21.1% Perplexity: 43347.943 (Cost: 651.993) Speed: 6545 wps\n",
      "21.9% Perplexity: 47034.365 (Cost: 484.392) Speed: 6555 wps\n",
      "22.6% Perplexity: 48828.921 (Cost: 337.970) Speed: 6563 wps\n",
      "23.4% Perplexity: 51517.990 (Cost: 650.699) Speed: 6572 wps\n",
      "24.1% Perplexity: 65199.324 (Cost: 858.968) Speed: 6578 wps\n",
      "24.9% Perplexity: 80870.864 (Cost: 616.823) Speed: 6586 wps\n",
      "25.6% Perplexity: 94284.369 (Cost: 345.884) Speed: 6594 wps\n",
      "26.4% Perplexity: 94957.167 (Cost: 346.425) Speed: 6601 wps\n",
      "27.1% Perplexity: 91542.542 (Cost: 318.766) Speed: 6606 wps\n",
      "27.9% Perplexity: 87980.986 (Cost: 454.621) Speed: 6612 wps\n",
      "28.6% Perplexity: 87435.623 (Cost: 299.427) Speed: 6618 wps\n",
      "29.4% Perplexity: 87800.366 (Cost: 319.142) Speed: 6624 wps\n",
      "30.1% Perplexity: 92176.545 (Cost: 319.937) Speed: 6629 wps\n",
      "30.9% Perplexity: 100312.557 (Cost: 675.162) Speed: 6634 wps\n",
      "31.7% Perplexity: 115312.159 (Cost: 574.528) Speed: 6641 wps\n",
      "32.4% Perplexity: 118822.851 (Cost: 384.127) Speed: 6646 wps\n",
      "33.2% Perplexity: 117635.807 (Cost: 346.676) Speed: 6652 wps\n",
      "33.9% Perplexity: 116611.910 (Cost: 537.840) Speed: 6656 wps\n",
      "34.7% Perplexity: 126404.581 (Cost: 536.833) Speed: 6660 wps\n",
      "35.4% Perplexity: 119227.044 (Cost: 273.483) Speed: 6663 wps\n",
      "36.2% Perplexity: 113610.285 (Cost: 370.883) Speed: 6666 wps\n",
      "36.9% Perplexity: 114250.804 (Cost: 386.818) Speed: 6669 wps\n",
      "37.7% Perplexity: 108687.788 (Cost: 273.999) Speed: 6672 wps\n",
      "38.4% Perplexity: 103963.013 (Cost: 266.306) Speed: 6677 wps\n",
      "39.2% Perplexity: 102124.115 (Cost: 336.462) Speed: 6679 wps\n",
      "39.9% Perplexity: 102513.799 (Cost: 298.060) Speed: 6681 wps\n",
      "40.7% Perplexity: 98411.327 (Cost: 320.891) Speed: 6684 wps\n",
      "41.4% Perplexity: 97232.648 (Cost: 397.605) Speed: 6687 wps\n",
      "42.2% Perplexity: 100963.952 (Cost: 477.193) Speed: 6690 wps\n",
      "43.0% Perplexity: 104333.571 (Cost: 650.704) Speed: 6692 wps\n",
      "43.7% Perplexity: 106682.337 (Cost: 428.422) Speed: 6694 wps\n",
      "44.5% Perplexity: 109563.784 (Cost: 497.929) Speed: 6696 wps\n",
      "45.2% Perplexity: 115748.692 (Cost: 563.993) Speed: 6699 wps\n",
      "46.0% Perplexity: 124151.069 (Cost: 788.799) Speed: 6700 wps\n",
      "46.7% Perplexity: 137603.579 (Cost: 470.726) Speed: 6703 wps\n",
      "47.5% Perplexity: 147638.313 (Cost: 474.826) Speed: 6705 wps\n",
      "48.2% Perplexity: 156391.739 (Cost: 424.833) Speed: 6707 wps\n",
      "49.0% Perplexity: 150459.535 (Cost: 271.662) Speed: 6711 wps\n",
      "49.7% Perplexity: 142455.451 (Cost: 311.997) Speed: 6712 wps\n",
      "50.5% Perplexity: 137155.812 (Cost: 258.075) Speed: 6713 wps\n",
      "51.2% Perplexity: 133288.105 (Cost: 349.208) Speed: 6714 wps\n",
      "52.0% Perplexity: 129550.945 (Cost: 321.861) Speed: 6716 wps\n",
      "52.8% Perplexity: 130793.231 (Cost: 369.419) Speed: 6718 wps\n",
      "53.5% Perplexity: 127421.156 (Cost: 307.867) Speed: 6720 wps\n",
      "54.3% Perplexity: 127836.790 (Cost: 340.301) Speed: 6722 wps\n",
      "55.0% Perplexity: 125982.524 (Cost: 292.042) Speed: 6723 wps\n",
      "55.8% Perplexity: 121588.461 (Cost: 311.269) Speed: 6724 wps\n",
      "56.5% Perplexity: 120281.976 (Cost: 400.337) Speed: 6726 wps\n",
      "57.3% Perplexity: 116636.426 (Cost: 282.807) Speed: 6728 wps\n",
      "58.0% Perplexity: 113201.374 (Cost: 303.858) Speed: 6729 wps\n",
      "58.8% Perplexity: 113930.302 (Cost: 377.695) Speed: 6730 wps\n",
      "59.5% Perplexity: 112604.975 (Cost: 518.674) Speed: 6731 wps\n",
      "60.3% Perplexity: 112962.805 (Cost: 346.553) Speed: 6732 wps\n",
      "61.0% Perplexity: 112104.021 (Cost: 642.637) Speed: 6734 wps\n",
      "61.8% Perplexity: 116439.726 (Cost: 497.356) Speed: 6735 wps\n",
      "62.5% Perplexity: 120116.979 (Cost: 493.629) Speed: 6736 wps\n",
      "63.3% Perplexity: 123096.353 (Cost: 455.114) Speed: 6738 wps\n",
      "64.1% Perplexity: 123577.376 (Cost: 336.499) Speed: 6738 wps\n",
      "64.8% Perplexity: 118733.812 (Cost: 268.128) Speed: 6739 wps\n",
      "65.6% Perplexity: 117112.512 (Cost: 443.248) Speed: 6740 wps\n",
      "66.3% Perplexity: 117038.745 (Cost: 351.606) Speed: 6740 wps\n",
      "67.1% Perplexity: 114243.395 (Cost: 378.259) Speed: 6741 wps\n",
      "67.8% Perplexity: 113625.914 (Cost: 320.233) Speed: 6742 wps\n",
      "68.6% Perplexity: 110758.069 (Cost: 269.238) Speed: 6743 wps\n",
      "69.3% Perplexity: 112580.214 (Cost: 288.183) Speed: 6744 wps\n",
      "70.1% Perplexity: 113119.986 (Cost: 289.446) Speed: 6746 wps\n",
      "70.8% Perplexity: 113039.093 (Cost: 306.201) Speed: 6746 wps\n",
      "71.6% Perplexity: 111440.572 (Cost: 319.309) Speed: 6747 wps\n",
      "72.3% Perplexity: 108236.278 (Cost: 335.627) Speed: 6748 wps\n",
      "73.1% Perplexity: 108964.114 (Cost: 323.849) Speed: 6749 wps\n",
      "73.9% Perplexity: 108905.723 (Cost: 368.179) Speed: 6750 wps\n",
      "74.6% Perplexity: 111737.164 (Cost: 523.585) Speed: 6750 wps\n",
      "75.4% Perplexity: 114721.650 (Cost: 425.339) Speed: 6752 wps\n",
      "76.1% Perplexity: 114157.184 (Cost: 417.634) Speed: 6753 wps\n",
      "76.9% Perplexity: 113318.379 (Cost: 672.570) Speed: 6753 wps\n",
      "77.6% Perplexity: 114109.660 (Cost: 388.762) Speed: 6754 wps\n",
      "78.4% Perplexity: 111547.714 (Cost: 291.513) Speed: 6755 wps\n",
      "79.1% Perplexity: 107630.709 (Cost: 272.083) Speed: 6756 wps\n",
      "79.9% Perplexity: 105441.073 (Cost: 295.961) Speed: 6757 wps\n",
      "80.6% Perplexity: 103728.605 (Cost: 322.857) Speed: 6758 wps\n",
      "81.4% Perplexity: 101462.144 (Cost: 277.245) Speed: 6758 wps\n",
      "82.1% Perplexity: 98010.112 (Cost: 258.530) Speed: 6758 wps\n",
      "82.9% Perplexity: 95070.926 (Cost: 317.513) Speed: 6759 wps\n",
      "83.6% Perplexity: 96968.119 (Cost: 701.459) Speed: 6759 wps\n",
      "84.4% Perplexity: 97026.140 (Cost: 567.513) Speed: 6760 wps\n",
      "85.2% Perplexity: 96116.061 (Cost: 318.519) Speed: 6761 wps\n",
      "85.9% Perplexity: 96460.450 (Cost: 681.512) Speed: 6762 wps\n",
      "86.7% Perplexity: 95417.926 (Cost: 418.410) Speed: 6762 wps\n",
      "87.4% Perplexity: 95250.001 (Cost: 357.144) Speed: 6763 wps\n",
      "88.2% Perplexity: 92723.665 (Cost: 327.465) Speed: 6763 wps\n",
      "88.9% Perplexity: 91259.161 (Cost: 283.605) Speed: 6764 wps\n",
      "89.7% Perplexity: 90904.888 (Cost: 416.583) Speed: 6764 wps\n",
      "90.4% Perplexity: 89895.833 (Cost: 395.289) Speed: 6765 wps\n",
      "91.2% Perplexity: 88372.244 (Cost: 310.539) Speed: 6765 wps\n",
      "91.9% Perplexity: 86023.302 (Cost: 270.269) Speed: 6766 wps\n",
      "92.7% Perplexity: 84039.341 (Cost: 304.772) Speed: 6766 wps\n",
      "93.4% Perplexity: 82182.005 (Cost: 347.166) Speed: 6767 wps\n",
      "94.2% Perplexity: 81095.599 (Cost: 292.810) Speed: 6767 wps\n",
      "95.0% Perplexity: 79256.536 (Cost: 294.370) Speed: 6767 wps\n",
      "95.7% Perplexity: 77229.760 (Cost: 256.471) Speed: 6767 wps\n",
      "96.5% Perplexity: 75555.332 (Cost: 289.854) Speed: 6768 wps\n",
      "97.2% Perplexity: 73963.175 (Cost: 277.950) Speed: 6769 wps\n",
      "98.0% Perplexity: 72978.555 (Cost: 302.426) Speed: 6769 wps\n",
      "98.7% Perplexity: 71914.793 (Cost: 293.010) Speed: 6770 wps\n",
      "99.5% Perplexity: 70606.277 (Cost: 443.173) Speed: 6770 wps\n",
      "Epoch: 1 Training Perplexity: 71099.120 (Cost: 11.172)\n",
      "Epoch: 1 Validation Perplexity: 77648.374 (Cost: 11.260)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'latest_filename' collides with 'save_path': 'checkpoint' and './mut1_model/checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2dc907e5a9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mvalid_perps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_perp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# run test pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         raise ValueError(\n\u001b[1;32m   1309\u001b[0m             \u001b[0;34m\"'latest_filename' collides with 'save_path': '%s' and '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             (latest_filename, save_path))\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'latest_filename' collides with 'save_path': 'checkpoint' and './mut1_model/checkpoint'"
     ]
    }
   ],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    # initial RNN state\n",
    "#     state = model.initial_state.eval()\n",
    "    state = sess.run(model.initial_state)\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        perplexity = np.exp(costs / iters)\n",
    "\n",
    "        if verbose and step % 10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"%.1f%% Perplexity: %.3f (Cost: %.3f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "\n",
    "    return (costs / iters), perplexity\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    learning_rate = 1.0\n",
    "    lr_decay = 0.8\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch\n",
    "\n",
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "cell_types = {\n",
    "    'lstm': VanillaLSTMCell,\n",
    "    'vanillaRNN': VanillaRNNCell,\n",
    "    'gru': GruCell,\n",
    "    'mut1': MUT1,\n",
    "    'mut2': MUT2\n",
    "#     'nig': NIGLSTMCell,\n",
    "#     'nfg': NFGLSTMCell,\n",
    "#     'nog': NOGLSTMCell,\n",
    "#     'niaf': NIAFLSTMCell,\n",
    "#     'noaf': NOAFLSTMCell,\n",
    "#     'np': NPLSTMCell,\n",
    "#     'cifg': CIFGLSTMCell,\n",
    "#     'fgr': FGRLSTMCell,\n",
    "}\n",
    "\n",
    "model_name = \"mut2\"\n",
    "CellType = cell_types[model_name]\n",
    "\n",
    "# CellType = rnn_cell.GRUCell\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "\n",
    "    # we create a separate model for validation and testing to alter the batch size and time steps\n",
    "    # reuse=True reuses variables from the previously defined `train_model`\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Epoch: %d Learning Rate: %.3f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.3f (Cost: %.3f)\" % (i + 1, train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.3f (Cost: %.3f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint')\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.3f (Cost: %.3f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = enumerate(ptb_reader.ptb_iterator(train_data, batch_size=20, num_steps=35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step, (x, y) = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
