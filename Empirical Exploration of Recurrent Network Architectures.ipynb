{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This article will explore a huge variety of recurrent neural network architectures. This article is mainly motivated by this paper \"An Empirical Exploration of Recurrent Network Architectures\" and we will implement the network architecures that were highlighted in the paper. \n",
    "\n",
    "The implementation will be using Google's Tensorflow and we will implement the code on top of the Recurrent Neural Network example in the tensorflow website and on top of the code written by Jim Flemming in this blog article here: https://github.com/fomorians/lstm-odyssey. I have updated the code based on the new tensorflow version (0.12 as of today) and the recurrent neural network architectures are based on the paper \"An Empirical Exploration of Recurrent Network Architectures\" http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "\n",
    "sequence_loss_by_example = tf.nn.seq2seq.sequence_loss_by_example\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "# import variants\n",
    "from variants.lstm import VanillaLSTMCell\n",
    "from variants.vanillaRNN import VanillaRNNCell\n",
    "# from variants.nig import NIGLSTMCell\n",
    "# from variants.nfg import NFGLSTMCell\n",
    "# from variants.nog import NOGLSTMCell\n",
    "# from variants.niaf import NIAFLSTMCell\n",
    "# from variants.noaf import NOAFLSTMCell\n",
    "# from variants.np import NPLSTMCell\n",
    "# from variants.cifg import CIFGLSTMCell\n",
    "# from variants.fgr import FGRLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../Machine175/AttentionModels/rnn/data/simple-examples/data/'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../../../Machine175/AttentionModels/rnn/data/simple-examples/data/'\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '.'\n",
    "checkpoint_path = '.'\n",
    "summary_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "\n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # initializer used for reusable variable initializer (see `get_variable`)\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "\n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "\n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "\n",
    "        self.final_state = states[-1]\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                                    [size, vocab_size],\n",
    "                                    initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape our target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "\n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "\n",
    "        if is_training:\n",
    "            # setup learning rate variable to decay\n",
    "            self.lr = tf.Variable(1.0, trainable=False)\n",
    "\n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n",
      "('y_prev shape is ', TensorShape([Dimension(20), Dimension(450)]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-70930bcf8e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# define our training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCellType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# we create a separate model for validation and testing to alter the batch size and time steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-210dc2645bd0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, CellType, is_training, config)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# define training operation and clip the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 \u001b[0min_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.pyc\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    582\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m   return (array_ops.reshape(math_ops.reduce_sum(grad, rx), sx),\n\u001b[0m\u001b[1;32m    585\u001b[0m           array_ops.reshape(math_ops.reduce_sum(grad, ry), sy))\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   2785\u001b[0m   result = _op_def_lib.apply_op(\"Sum\", input=input,\n\u001b[1;32m   2786\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   2788\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    # initial RNN state\n",
    "#     state = model.initial_state.eval()\n",
    "    state = sess.run(model.initial_state)\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        perplexity = np.exp(costs / iters)\n",
    "\n",
    "        if verbose and step % 10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"%.1f%% Perplexity: %.3f (Cost: %.3f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "\n",
    "    return (costs / iters), perplexity\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    learning_rate = 1.0\n",
    "    lr_decay = 0.8\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch\n",
    "\n",
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "cell_types = {\n",
    "    'lstm': VanillaLSTMCell,\n",
    "    'vanillaRNN': VanillaRNNCell\n",
    "#     'nig': NIGLSTMCell,\n",
    "#     'nfg': NFGLSTMCell,\n",
    "#     'nog': NOGLSTMCell,\n",
    "#     'niaf': NIAFLSTMCell,\n",
    "#     'noaf': NOAFLSTMCell,\n",
    "#     'np': NPLSTMCell,\n",
    "#     'cifg': CIFGLSTMCell,\n",
    "#     'fgr': FGRLSTMCell,\n",
    "}\n",
    "\n",
    "model_name = \"lstm\"\n",
    "CellType = cell_types[model_name]\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "\n",
    "    # we create a separate model for validation and testing to alter the batch size and time steps\n",
    "    # reuse=True reuses variables from the previously defined `train_model`\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Epoch: %d Learning Rate: %.3f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.3f (Cost: %.3f)\" % (i + 1, train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.3f (Cost: %.3f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint')\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.3f (Cost: %.3f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = enumerate(ptb_reader.ptb_iterator(train_data, batch_size=20, num_steps=35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step, (x, y) = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
