{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This article will explore a huge variety of recurrent neural network architectures. This article is mainly motivated by this paper \"An Empirical Exploration of Recurrent Network Architectures\" and we will implement the network architecures that were highlighted in the paper. \n",
    "\n",
    "The implementation will be using Google's Tensorflow and we will implement the code on top of the Recurrent Neural Network example in the tensorflow website and on top of the code written by Jim Flemming in this blog article here: https://github.com/fomorians/lstm-odyssey. I have updated the code based on the new tensorflow version (0.12 as of today) and the recurrent neural network architectures are based on the paper \"An Empirical Exploration of Recurrent Network Architectures\" http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.nn.rnn_cell import GRUCell\n",
    "\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "\n",
    "sequence_loss_by_example = tf.nn.seq2seq.sequence_loss_by_example\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "# import variants\n",
    "from variants.lstm import VanillaLSTMCell\n",
    "from variants.vanillaRNN import VanillaRNNCell\n",
    "from variants.gru import GruCell\n",
    "from variants.mut1 import MUT1\n",
    "from variants.mut2 import MUT2\n",
    "from variants.mut3 import MUT3\n",
    "from variants.nig import NIGLSTMCell\n",
    "from variants.nfg import NFGLSTMCell\n",
    "from variants.nog import NOGLSTMCell\n",
    "from variants.niaf import NIAFLSTMCell\n",
    "from variants.noaf import NOAFLSTMCell\n",
    "from variants.np import NPLSTMCell\n",
    "from variants.cifg import CIFGLSTMCell\n",
    "from variants.fgr import FGRLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../../../Machine175/AttentionModels/rnn/data/simple-examples/data/'\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "\n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # initializer used for reusable variable initializer (see `get_variable`)\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "\n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "\n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "\n",
    "        self.final_state = states[-1]\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                                    [size, vocab_size],\n",
    "                                    initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape our target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "\n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "\n",
    "        if is_training:\n",
    "            # setup learning rate variable to decay\n",
    "            self.lr = tf.Variable(1.0, trainable=False)\n",
    "\n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning Rate: 1.000\n",
      "0.0% Perplexity: 10005.389 (Cost: 322.381) Speed: 549 wps\n",
      "0.8% Perplexity: 54038.300 (Cost: 335.576) Speed: 3075 wps\n",
      "1.5% Perplexity: 112216.911 (Cost: 318.472) Speed: 4022 wps\n",
      "2.3% Perplexity: 146758.336 (Cost: 390.706) Speed: 4529 wps\n",
      "3.0% Perplexity: 223129.943 (Cost: 444.391) Speed: 4831 wps\n",
      "3.8% Perplexity: 162859.898 (Cost: 361.191) Speed: 5043 wps\n",
      "4.5% Perplexity: 127699.653 (Cost: 315.688) Speed: 5184 wps\n",
      "5.3% Perplexity: 95390.743 (Cost: 282.979) Speed: 5295 wps\n",
      "6.0% Perplexity: 69915.618 (Cost: 315.919) Speed: 5381 wps\n",
      "6.8% Perplexity: 56307.945 (Cost: 297.597) Speed: 5454 wps\n",
      "7.5% Perplexity: 48276.154 (Cost: 295.391) Speed: 5516 wps\n",
      "8.3% Perplexity: 41534.507 (Cost: 312.603) Speed: 5569 wps\n",
      "9.0% Perplexity: 42836.100 (Cost: 305.908) Speed: 5609 wps\n",
      "9.8% Perplexity: 36439.318 (Cost: 270.570) Speed: 5644 wps\n",
      "10.6% Perplexity: 31390.108 (Cost: 271.835) Speed: 5672 wps\n",
      "11.3% Perplexity: 28021.258 (Cost: 269.930) Speed: 5701 wps\n",
      "12.1% Perplexity: 24631.403 (Cost: 248.965) Speed: 5721 wps\n",
      "12.8% Perplexity: 22219.180 (Cost: 321.548) Speed: 5740 wps\n",
      "13.6% Perplexity: 20146.866 (Cost: 321.111) Speed: 5758 wps\n",
      "14.3% Perplexity: 18523.141 (Cost: 281.002) Speed: 5772 wps\n",
      "15.1% Perplexity: 16815.179 (Cost: 278.806) Speed: 5786 wps\n",
      "15.8% Perplexity: 15798.192 (Cost: 269.956) Speed: 5802 wps\n",
      "16.6% Perplexity: 14410.128 (Cost: 276.249) Speed: 5814 wps\n",
      "17.3% Perplexity: 13368.622 (Cost: 274.769) Speed: 5824 wps\n",
      "18.1% Perplexity: 12206.030 (Cost: 261.450) Speed: 5834 wps\n",
      "18.8% Perplexity: 11387.597 (Cost: 260.507) Speed: 5841 wps\n",
      "19.6% Perplexity: 10524.301 (Cost: 255.252) Speed: 5851 wps\n",
      "20.3% Perplexity: 9802.617 (Cost: 248.583) Speed: 5859 wps\n",
      "21.1% Perplexity: 9257.629 (Cost: 248.876) Speed: 5868 wps\n",
      "21.9% Perplexity: 8663.149 (Cost: 253.251) Speed: 5879 wps\n",
      "22.6% Perplexity: 8154.030 (Cost: 233.211) Speed: 5886 wps\n",
      "23.4% Perplexity: 7692.653 (Cost: 257.596) Speed: 5894 wps\n",
      "24.1% Perplexity: 7230.843 (Cost: 221.351) Speed: 5898 wps\n",
      "24.9% Perplexity: 6860.608 (Cost: 256.928) Speed: 5904 wps\n",
      "25.6% Perplexity: 6512.396 (Cost: 256.674) Speed: 5909 wps\n",
      "26.4% Perplexity: 6189.291 (Cost: 231.626) Speed: 5914 wps\n",
      "27.1% Perplexity: 5892.793 (Cost: 246.908) Speed: 5920 wps\n",
      "27.9% Perplexity: 5603.880 (Cost: 248.912) Speed: 5925 wps\n",
      "28.6% Perplexity: 5354.148 (Cost: 230.073) Speed: 5930 wps\n",
      "29.4% Perplexity: 5138.356 (Cost: 252.858) Speed: 5934 wps\n",
      "30.1% Perplexity: 4937.779 (Cost: 242.151) Speed: 5938 wps\n",
      "30.9% Perplexity: 4743.867 (Cost: 252.850) Speed: 5941 wps\n",
      "31.7% Perplexity: 4567.092 (Cost: 243.030) Speed: 5943 wps\n",
      "32.4% Perplexity: 4377.313 (Cost: 223.942) Speed: 5947 wps\n",
      "33.2% Perplexity: 4212.431 (Cost: 224.025) Speed: 5951 wps\n",
      "33.9% Perplexity: 4055.129 (Cost: 222.917) Speed: 5956 wps\n",
      "34.7% Perplexity: 3917.462 (Cost: 242.573) Speed: 5959 wps\n",
      "35.4% Perplexity: 3785.248 (Cost: 229.654) Speed: 5962 wps\n",
      "36.2% Perplexity: 3649.746 (Cost: 233.090) Speed: 5964 wps\n",
      "36.9% Perplexity: 3532.711 (Cost: 235.619) Speed: 5966 wps\n",
      "37.7% Perplexity: 3425.137 (Cost: 222.239) Speed: 5969 wps\n",
      "38.4% Perplexity: 3336.484 (Cost: 233.150) Speed: 5971 wps\n",
      "39.2% Perplexity: 3225.367 (Cost: 223.291) Speed: 5974 wps\n",
      "39.9% Perplexity: 3127.516 (Cost: 215.912) Speed: 5977 wps\n",
      "40.7% Perplexity: 3048.844 (Cost: 232.902) Speed: 5980 wps\n",
      "41.4% Perplexity: 2963.207 (Cost: 221.141) Speed: 5983 wps\n",
      "42.2% Perplexity: 2879.364 (Cost: 228.354) Speed: 5985 wps\n",
      "43.0% Perplexity: 2795.804 (Cost: 213.138) Speed: 5986 wps\n",
      "43.7% Perplexity: 2719.479 (Cost: 229.995) Speed: 5989 wps\n",
      "44.5% Perplexity: 2657.630 (Cost: 230.316) Speed: 5991 wps\n",
      "45.2% Perplexity: 2598.101 (Cost: 232.261) Speed: 5991 wps\n",
      "46.0% Perplexity: 2539.984 (Cost: 226.724) Speed: 5994 wps\n",
      "46.7% Perplexity: 2481.709 (Cost: 219.834) Speed: 5997 wps\n",
      "47.5% Perplexity: 2424.114 (Cost: 228.926) Speed: 5999 wps\n",
      "48.2% Perplexity: 2370.548 (Cost: 216.637) Speed: 6001 wps\n",
      "49.0% Perplexity: 2316.598 (Cost: 216.619) Speed: 6003 wps\n",
      "49.7% Perplexity: 2266.864 (Cost: 221.602) Speed: 6004 wps\n",
      "50.5% Perplexity: 2210.570 (Cost: 208.112) Speed: 6005 wps\n",
      "51.2% Perplexity: 2160.555 (Cost: 216.536) Speed: 6006 wps\n",
      "52.0% Perplexity: 2116.195 (Cost: 219.458) Speed: 6007 wps\n",
      "52.8% Perplexity: 2068.864 (Cost: 216.488) Speed: 6008 wps\n",
      "53.5% Perplexity: 2026.651 (Cost: 210.824) Speed: 6010 wps\n",
      "54.3% Perplexity: 1985.435 (Cost: 208.932) Speed: 6012 wps\n",
      "55.0% Perplexity: 1941.648 (Cost: 209.320) Speed: 6014 wps\n",
      "55.8% Perplexity: 1904.417 (Cost: 215.120) Speed: 6016 wps\n",
      "56.5% Perplexity: 1870.869 (Cost: 216.696) Speed: 6017 wps\n",
      "57.3% Perplexity: 1836.504 (Cost: 208.355) Speed: 6018 wps\n",
      "58.0% Perplexity: 1801.005 (Cost: 213.692) Speed: 6020 wps\n",
      "58.8% Perplexity: 1765.161 (Cost: 198.323) Speed: 6021 wps\n",
      "59.5% Perplexity: 1735.641 (Cost: 213.673) Speed: 6021 wps\n",
      "60.3% Perplexity: 1705.646 (Cost: 210.384) Speed: 6022 wps\n",
      "61.0% Perplexity: 1677.781 (Cost: 214.880) Speed: 6023 wps\n",
      "61.8% Perplexity: 1647.128 (Cost: 214.465) Speed: 6024 wps\n",
      "62.5% Perplexity: 1618.363 (Cost: 210.705) Speed: 6025 wps\n",
      "63.3% Perplexity: 1593.621 (Cost: 211.478) Speed: 6026 wps\n",
      "64.1% Perplexity: 1567.513 (Cost: 209.509) Speed: 6027 wps\n",
      "64.8% Perplexity: 1541.948 (Cost: 203.453) Speed: 6028 wps\n",
      "65.6% Perplexity: 1517.219 (Cost: 193.638) Speed: 6028 wps\n",
      "66.3% Perplexity: 1492.327 (Cost: 198.569) Speed: 6029 wps\n",
      "67.1% Perplexity: 1471.601 (Cost: 212.862) Speed: 6030 wps\n",
      "67.8% Perplexity: 1451.647 (Cost: 210.303) Speed: 6030 wps\n",
      "68.6% Perplexity: 1430.482 (Cost: 211.253) Speed: 6031 wps\n",
      "69.3% Perplexity: 1410.757 (Cost: 214.602) Speed: 6032 wps\n",
      "70.1% Perplexity: 1391.038 (Cost: 220.413) Speed: 6032 wps\n",
      "70.8% Perplexity: 1372.816 (Cost: 207.925) Speed: 6034 wps\n",
      "71.6% Perplexity: 1354.903 (Cost: 211.923) Speed: 6035 wps\n",
      "72.3% Perplexity: 1338.313 (Cost: 204.857) Speed: 6035 wps\n",
      "73.1% Perplexity: 1320.421 (Cost: 210.943) Speed: 6036 wps\n",
      "73.9% Perplexity: 1304.248 (Cost: 208.139) Speed: 6036 wps\n",
      "74.6% Perplexity: 1286.076 (Cost: 208.008) Speed: 6037 wps\n",
      "75.4% Perplexity: 1268.670 (Cost: 200.701) Speed: 6037 wps\n",
      "76.1% Perplexity: 1254.401 (Cost: 208.282) Speed: 6038 wps\n",
      "76.9% Perplexity: 1239.302 (Cost: 202.166) Speed: 6039 wps\n",
      "77.6% Perplexity: 1225.821 (Cost: 203.016) Speed: 6040 wps\n",
      "78.4% Perplexity: 1209.914 (Cost: 196.414) Speed: 6041 wps\n",
      "79.1% Perplexity: 1194.569 (Cost: 201.780) Speed: 6042 wps\n",
      "79.9% Perplexity: 1180.741 (Cost: 198.531) Speed: 6042 wps\n",
      "80.6% Perplexity: 1165.530 (Cost: 193.592) Speed: 6042 wps\n",
      "81.4% Perplexity: 1150.195 (Cost: 201.027) Speed: 6043 wps\n",
      "82.1% Perplexity: 1135.824 (Cost: 207.387) Speed: 6043 wps\n",
      "82.9% Perplexity: 1124.336 (Cost: 210.505) Speed: 6044 wps\n",
      "83.6% Perplexity: 1113.248 (Cost: 209.486) Speed: 6045 wps\n",
      "84.4% Perplexity: 1102.235 (Cost: 199.542) Speed: 6045 wps\n",
      "85.2% Perplexity: 1090.764 (Cost: 203.533) Speed: 6045 wps\n",
      "85.9% Perplexity: 1079.524 (Cost: 208.129) Speed: 6046 wps\n",
      "86.7% Perplexity: 1067.876 (Cost: 185.235) Speed: 6046 wps\n",
      "87.4% Perplexity: 1057.631 (Cost: 196.693) Speed: 6046 wps\n",
      "88.2% Perplexity: 1045.867 (Cost: 208.004) Speed: 6047 wps\n",
      "88.9% Perplexity: 1036.156 (Cost: 196.822) Speed: 6047 wps\n",
      "89.7% Perplexity: 1024.162 (Cost: 195.945) Speed: 6047 wps\n",
      "90.4% Perplexity: 1012.750 (Cost: 199.004) Speed: 6047 wps\n",
      "91.2% Perplexity: 1001.310 (Cost: 195.228) Speed: 6048 wps\n",
      "91.9% Perplexity: 990.555 (Cost: 206.015) Speed: 6049 wps\n",
      "92.7% Perplexity: 980.403 (Cost: 197.596) Speed: 6049 wps\n",
      "93.4% Perplexity: 971.990 (Cost: 206.686) Speed: 6050 wps\n",
      "94.2% Perplexity: 962.372 (Cost: 199.362) Speed: 6050 wps\n",
      "95.0% Perplexity: 953.163 (Cost: 197.913) Speed: 6051 wps\n",
      "95.7% Perplexity: 944.025 (Cost: 198.076) Speed: 6052 wps\n",
      "96.5% Perplexity: 935.729 (Cost: 206.321) Speed: 6052 wps\n",
      "97.2% Perplexity: 928.735 (Cost: 215.088) Speed: 6052 wps\n",
      "98.0% Perplexity: 922.146 (Cost: 204.612) Speed: 6052 wps\n",
      "98.7% Perplexity: 914.423 (Cost: 197.743) Speed: 6053 wps\n",
      "99.5% Perplexity: 907.913 (Cost: 207.103) Speed: 6054 wps\n",
      "Epoch: 1 Training Perplexity: 903.927 (Cost: 6.807)\n",
      "Epoch: 1 Validation Perplexity: 260.083 (Cost: 5.561)\n",
      "Epoch: 2 Learning Rate: 1.000\n",
      "0.0% Perplexity: 392.195 (Cost: 209.012) Speed: 3347 wps\n",
      "0.8% Perplexity: 360.692 (Cost: 207.932) Speed: 5262 wps\n",
      "1.5% Perplexity: 331.550 (Cost: 206.097) Speed: 5621 wps\n",
      "2.3% Perplexity: 325.425 (Cost: 194.310) Speed: 5772 wps\n",
      "3.0% Perplexity: 320.217 (Cost: 203.581) Speed: 5859 wps\n",
      "3.8% Perplexity: 309.572 (Cost: 195.466) Speed: 5915 wps\n",
      "4.5% Perplexity: 307.471 (Cost: 199.614) Speed: 5936 wps\n",
      "5.3% Perplexity: 298.939 (Cost: 197.401) Speed: 5949 wps\n",
      "6.0% Perplexity: 293.668 (Cost: 192.899) Speed: 5971 wps\n",
      "6.8% Perplexity: 288.598 (Cost: 193.376) Speed: 5988 wps\n",
      "7.5% Perplexity: 282.309 (Cost: 187.207) Speed: 5999 wps\n",
      "8.3% Perplexity: 281.501 (Cost: 195.998) Speed: 6013 wps\n",
      "9.0% Perplexity: 278.479 (Cost: 192.414) Speed: 6018 wps\n",
      "9.8% Perplexity: 277.607 (Cost: 193.163) Speed: 6024 wps\n",
      "10.6% Perplexity: 279.881 (Cost: 200.791) Speed: 6031 wps\n",
      "11.3% Perplexity: 282.337 (Cost: 208.323) Speed: 6035 wps\n",
      "12.1% Perplexity: 283.601 (Cost: 195.387) Speed: 6036 wps\n",
      "12.8% Perplexity: 286.343 (Cost: 202.755) Speed: 6039 wps\n",
      "13.6% Perplexity: 287.851 (Cost: 201.210) Speed: 6047 wps\n",
      "14.3% Perplexity: 288.412 (Cost: 197.893) Speed: 6051 wps\n",
      "15.1% Perplexity: 289.063 (Cost: 198.449) Speed: 6053 wps\n",
      "15.8% Perplexity: 290.121 (Cost: 192.626) Speed: 6058 wps\n",
      "16.6% Perplexity: 289.291 (Cost: 198.667) Speed: 6059 wps\n",
      "17.3% Perplexity: 289.398 (Cost: 192.155) Speed: 6061 wps\n",
      "18.1% Perplexity: 288.424 (Cost: 192.649) Speed: 6062 wps\n",
      "18.8% Perplexity: 288.935 (Cost: 197.300) Speed: 6062 wps\n",
      "19.6% Perplexity: 289.277 (Cost: 199.320) Speed: 6063 wps\n",
      "20.3% Perplexity: 290.067 (Cost: 202.815) Speed: 6063 wps\n",
      "21.1% Perplexity: 289.774 (Cost: 185.558) Speed: 6067 wps\n",
      "21.9% Perplexity: 289.763 (Cost: 197.547) Speed: 6069 wps\n",
      "22.6% Perplexity: 287.904 (Cost: 189.325) Speed: 6069 wps\n",
      "23.4% Perplexity: 286.071 (Cost: 188.700) Speed: 6070 wps\n",
      "24.1% Perplexity: 283.498 (Cost: 183.302) Speed: 6071 wps\n",
      "24.9% Perplexity: 282.564 (Cost: 198.953) Speed: 6070 wps\n",
      "25.6% Perplexity: 283.843 (Cost: 209.480) Speed: 6070 wps\n",
      "26.4% Perplexity: 285.319 (Cost: 194.050) Speed: 6070 wps\n",
      "27.1% Perplexity: 284.731 (Cost: 191.125) Speed: 6072 wps\n",
      "27.9% Perplexity: 283.489 (Cost: 206.927) Speed: 6072 wps\n",
      "28.6% Perplexity: 282.912 (Cost: 180.071) Speed: 6073 wps\n",
      "29.4% Perplexity: 282.736 (Cost: 206.647) Speed: 6074 wps\n",
      "30.1% Perplexity: 282.533 (Cost: 199.386) Speed: 6073 wps\n",
      "30.9% Perplexity: 281.503 (Cost: 197.155) Speed: 6074 wps\n",
      "31.7% Perplexity: 281.290 (Cost: 200.108) Speed: 6073 wps\n",
      "32.4% Perplexity: 280.750 (Cost: 189.042) Speed: 6074 wps\n",
      "33.2% Perplexity: 280.240 (Cost: 185.085) Speed: 6073 wps\n",
      "33.9% Perplexity: 279.769 (Cost: 192.699) Speed: 6075 wps\n",
      "34.7% Perplexity: 279.371 (Cost: 203.541) Speed: 6075 wps\n",
      "35.4% Perplexity: 278.937 (Cost: 198.953) Speed: 6075 wps\n",
      "36.2% Perplexity: 277.805 (Cost: 199.352) Speed: 6076 wps\n",
      "36.9% Perplexity: 277.395 (Cost: 203.159) Speed: 6078 wps\n",
      "37.7% Perplexity: 277.651 (Cost: 200.650) Speed: 6077 wps\n",
      "38.4% Perplexity: 278.229 (Cost: 193.681) Speed: 6077 wps\n",
      "39.2% Perplexity: 277.431 (Cost: 190.523) Speed: 6078 wps\n",
      "39.9% Perplexity: 276.819 (Cost: 186.787) Speed: 6078 wps\n",
      "40.7% Perplexity: 277.584 (Cost: 202.465) Speed: 6078 wps\n",
      "41.4% Perplexity: 276.895 (Cost: 184.643) Speed: 6080 wps\n",
      "42.2% Perplexity: 276.254 (Cost: 191.644) Speed: 6081 wps\n",
      "43.0% Perplexity: 274.964 (Cost: 182.628) Speed: 6082 wps\n",
      "43.7% Perplexity: 274.296 (Cost: 197.167) Speed: 6082 wps\n",
      "44.5% Perplexity: 274.595 (Cost: 200.403) Speed: 6083 wps\n",
      "45.2% Perplexity: 275.060 (Cost: 199.772) Speed: 6083 wps\n",
      "46.0% Perplexity: 275.270 (Cost: 193.886) Speed: 6082 wps\n",
      "46.7% Perplexity: 275.398 (Cost: 189.421) Speed: 6082 wps\n",
      "47.5% Perplexity: 275.059 (Cost: 202.383) Speed: 6082 wps\n",
      "48.2% Perplexity: 274.781 (Cost: 189.638) Speed: 6082 wps\n",
      "49.0% Perplexity: 274.392 (Cost: 184.947) Speed: 6082 wps\n",
      "49.7% Perplexity: 274.014 (Cost: 187.688) Speed: 6084 wps\n",
      "50.5% Perplexity: 272.558 (Cost: 177.983) Speed: 6085 wps\n",
      "51.2% Perplexity: 271.811 (Cost: 196.013) Speed: 6084 wps\n",
      "52.0% Perplexity: 271.490 (Cost: 191.780) Speed: 6085 wps\n",
      "52.8% Perplexity: 270.754 (Cost: 192.053) Speed: 6085 wps\n",
      "53.5% Perplexity: 270.566 (Cost: 187.318) Speed: 6084 wps\n",
      "54.3% Perplexity: 269.987 (Cost: 183.316) Speed: 6084 wps\n",
      "55.0% Perplexity: 268.776 (Cost: 181.606) Speed: 6084 wps\n",
      "55.8% Perplexity: 268.252 (Cost: 185.054) Speed: 6083 wps\n",
      "56.5% Perplexity: 267.700 (Cost: 188.319) Speed: 6084 wps\n",
      "57.3% Perplexity: 266.714 (Cost: 186.497) Speed: 6085 wps\n",
      "58.0% Perplexity: 265.926 (Cost: 187.857) Speed: 6086 wps\n",
      "58.8% Perplexity: 265.086 (Cost: 176.062) Speed: 6086 wps\n",
      "59.5% Perplexity: 264.695 (Cost: 194.830) Speed: 6087 wps\n",
      "60.3% Perplexity: 264.365 (Cost: 193.809) Speed: 6087 wps\n",
      "61.0% Perplexity: 264.164 (Cost: 189.690) Speed: 6087 wps\n",
      "61.8% Perplexity: 263.300 (Cost: 195.047) Speed: 6087 wps\n",
      "62.5% Perplexity: 262.622 (Cost: 190.727) Speed: 6087 wps\n",
      "63.3% Perplexity: 262.509 (Cost: 194.189) Speed: 6087 wps\n",
      "64.1% Perplexity: 262.059 (Cost: 185.650) Speed: 6086 wps\n",
      "64.8% Perplexity: 261.484 (Cost: 180.657) Speed: 6087 wps\n",
      "65.6% Perplexity: 260.805 (Cost: 176.442) Speed: 6088 wps\n",
      "66.3% Perplexity: 260.095 (Cost: 182.265) Speed: 6088 wps\n",
      "67.1% Perplexity: 260.144 (Cost: 200.047) Speed: 6088 wps\n",
      "67.8% Perplexity: 260.076 (Cost: 190.241) Speed: 6089 wps\n",
      "68.6% Perplexity: 259.623 (Cost: 196.902) Speed: 6089 wps\n",
      "69.3% Perplexity: 259.305 (Cost: 191.968) Speed: 6089 wps\n",
      "70.1% Perplexity: 258.961 (Cost: 206.767) Speed: 6088 wps\n",
      "70.8% Perplexity: 258.821 (Cost: 190.897) Speed: 6088 wps\n",
      "71.6% Perplexity: 258.651 (Cost: 195.228) Speed: 6088 wps\n",
      "72.3% Perplexity: 258.733 (Cost: 189.046) Speed: 6088 wps\n",
      "73.1% Perplexity: 258.499 (Cost: 195.277) Speed: 6089 wps\n",
      "73.9% Perplexity: 258.382 (Cost: 192.455) Speed: 6090 wps\n",
      "74.6% Perplexity: 257.793 (Cost: 194.623) Speed: 6090 wps\n",
      "75.4% Perplexity: 257.140 (Cost: 188.410) Speed: 6090 wps\n",
      "76.1% Perplexity: 257.105 (Cost: 197.746) Speed: 6090 wps\n",
      "76.9% Perplexity: 256.864 (Cost: 185.745) Speed: 6090 wps\n",
      "77.6% Perplexity: 256.828 (Cost: 192.989) Speed: 6090 wps\n",
      "78.4% Perplexity: 256.053 (Cost: 175.262) Speed: 6089 wps\n",
      "79.1% Perplexity: 255.431 (Cost: 187.467) Speed: 6089 wps\n",
      "79.9% Perplexity: 255.043 (Cost: 178.308) Speed: 6089 wps\n",
      "80.6% Perplexity: 254.212 (Cost: 177.195) Speed: 6089 wps\n",
      "81.4% Perplexity: 253.337 (Cost: 184.889) Speed: 6090 wps\n",
      "82.1% Perplexity: 252.482 (Cost: 190.261) Speed: 6090 wps\n",
      "82.9% Perplexity: 252.092 (Cost: 189.203) Speed: 6091 wps\n",
      "83.6% Perplexity: 251.931 (Cost: 189.581) Speed: 6091 wps\n",
      "84.4% Perplexity: 251.778 (Cost: 188.922) Speed: 6090 wps\n",
      "85.2% Perplexity: 251.464 (Cost: 183.846) Speed: 6090 wps\n",
      "85.9% Perplexity: 251.259 (Cost: 195.554) Speed: 6090 wps\n",
      "86.7% Perplexity: 250.674 (Cost: 169.949) Speed: 6089 wps\n",
      "87.4% Perplexity: 250.375 (Cost: 184.438) Speed: 6090 wps\n",
      "88.2% Perplexity: 249.791 (Cost: 191.938) Speed: 6091 wps\n",
      "88.9% Perplexity: 249.559 (Cost: 177.618) Speed: 6091 wps\n",
      "89.7% Perplexity: 248.826 (Cost: 186.008) Speed: 6091 wps\n",
      "90.4% Perplexity: 248.140 (Cost: 183.489) Speed: 6091 wps\n",
      "91.2% Perplexity: 247.383 (Cost: 180.698) Speed: 6091 wps\n",
      "91.9% Perplexity: 246.751 (Cost: 189.717) Speed: 6091 wps\n",
      "92.7% Perplexity: 246.138 (Cost: 185.596) Speed: 6091 wps\n",
      "93.4% Perplexity: 246.037 (Cost: 194.815) Speed: 6090 wps\n",
      "94.2% Perplexity: 245.493 (Cost: 183.567) Speed: 6091 wps\n",
      "95.0% Perplexity: 245.012 (Cost: 184.658) Speed: 6091 wps\n",
      "95.7% Perplexity: 244.570 (Cost: 180.766) Speed: 6091 wps\n",
      "96.5% Perplexity: 244.176 (Cost: 191.608) Speed: 6092 wps\n",
      "97.2% Perplexity: 244.094 (Cost: 197.036) Speed: 6092 wps\n",
      "98.0% Perplexity: 244.195 (Cost: 191.115) Speed: 6092 wps\n",
      "98.7% Perplexity: 243.999 (Cost: 187.216) Speed: 6092 wps\n",
      "99.5% Perplexity: 244.022 (Cost: 194.217) Speed: 6092 wps\n",
      "Epoch: 2 Training Perplexity: 244.037 (Cost: 5.497)\n",
      "Epoch: 2 Validation Perplexity: 192.232 (Cost: 5.259)\n",
      "Epoch: 3 Learning Rate: 1.000\n",
      "0.0% Perplexity: 294.647 (Cost: 199.002) Speed: 2786 wps\n",
      "0.8% Perplexity: 248.973 (Cost: 191.860) Speed: 5084 wps\n",
      "1.5% Perplexity: 224.180 (Cost: 192.644) Speed: 5521 wps\n",
      "2.3% Perplexity: 219.911 (Cost: 181.406) Speed: 5703 wps\n",
      "3.0% Perplexity: 216.928 (Cost: 189.634) Speed: 5789 wps\n",
      "3.8% Perplexity: 212.225 (Cost: 184.838) Speed: 5841 wps\n",
      "4.5% Perplexity: 210.992 (Cost: 189.478) Speed: 5886 wps\n",
      "5.3% Perplexity: 207.603 (Cost: 186.778) Speed: 5912 wps\n",
      "6.0% Perplexity: 203.279 (Cost: 177.919) Speed: 5932 wps\n",
      "6.8% Perplexity: 199.342 (Cost: 182.630) Speed: 5942 wps\n",
      "7.5% Perplexity: 195.626 (Cost: 174.192) Speed: 5952 wps\n",
      "8.3% Perplexity: 195.238 (Cost: 184.226) Speed: 5964 wps\n",
      "9.0% Perplexity: 193.889 (Cost: 178.902) Speed: 5976 wps\n",
      "9.8% Perplexity: 194.040 (Cost: 181.596) Speed: 5985 wps\n",
      "10.6% Perplexity: 195.712 (Cost: 185.956) Speed: 5991 wps\n",
      "11.3% Perplexity: 197.699 (Cost: 191.542) Speed: 5996 wps\n",
      "12.1% Perplexity: 199.243 (Cost: 184.061) Speed: 6004 wps\n",
      "12.8% Perplexity: 201.257 (Cost: 192.266) Speed: 6013 wps\n",
      "13.6% Perplexity: 202.728 (Cost: 188.953) Speed: 6014 wps\n",
      "14.3% Perplexity: 203.472 (Cost: 187.279) Speed: 6020 wps\n",
      "15.1% Perplexity: 204.482 (Cost: 189.514) Speed: 6025 wps\n",
      "15.8% Perplexity: 205.490 (Cost: 184.000) Speed: 6026 wps\n",
      "16.6% Perplexity: 205.530 (Cost: 186.864) Speed: 6028 wps\n",
      "17.3% Perplexity: 206.157 (Cost: 184.062) Speed: 6031 wps\n",
      "18.1% Perplexity: 205.690 (Cost: 181.919) Speed: 6032 wps\n",
      "18.8% Perplexity: 206.303 (Cost: 187.034) Speed: 6037 wps\n",
      "19.6% Perplexity: 206.853 (Cost: 189.532) Speed: 6042 wps\n",
      "20.3% Perplexity: 207.520 (Cost: 191.991) Speed: 6043 wps\n",
      "21.1% Perplexity: 207.643 (Cost: 179.872) Speed: 6043 wps\n",
      "21.9% Perplexity: 207.940 (Cost: 190.127) Speed: 6045 wps\n",
      "22.6% Perplexity: 206.769 (Cost: 177.978) Speed: 6045 wps\n",
      "23.4% Perplexity: 205.222 (Cost: 178.942) Speed: 6045 wps\n",
      "24.1% Perplexity: 203.609 (Cost: 174.324) Speed: 6045 wps\n",
      "24.9% Perplexity: 203.081 (Cost: 187.749) Speed: 6046 wps\n",
      "25.6% Perplexity: 204.011 (Cost: 197.733) Speed: 6047 wps\n",
      "26.4% Perplexity: 205.432 (Cost: 185.473) Speed: 6049 wps\n",
      "27.1% Perplexity: 205.321 (Cost: 179.055) Speed: 6052 wps\n",
      "27.9% Perplexity: 204.628 (Cost: 196.798) Speed: 6055 wps\n",
      "28.6% Perplexity: 204.105 (Cost: 171.866) Speed: 6056 wps\n",
      "29.4% Perplexity: 204.187 (Cost: 193.527) Speed: 6056 wps\n",
      "30.1% Perplexity: 204.274 (Cost: 187.656) Speed: 6056 wps\n",
      "30.9% Perplexity: 203.450 (Cost: 184.380) Speed: 6055 wps\n",
      "31.7% Perplexity: 203.448 (Cost: 190.596) Speed: 6055 wps\n",
      "32.4% Perplexity: 203.067 (Cost: 183.640) Speed: 6057 wps\n",
      "33.2% Perplexity: 203.000 (Cost: 174.585) Speed: 6058 wps\n",
      "33.9% Perplexity: 202.856 (Cost: 184.510) Speed: 6057 wps\n",
      "34.7% Perplexity: 202.761 (Cost: 193.686) Speed: 6057 wps\n",
      "35.4% Perplexity: 202.626 (Cost: 187.105) Speed: 6057 wps\n",
      "36.2% Perplexity: 202.089 (Cost: 191.558) Speed: 6056 wps\n",
      "36.9% Perplexity: 201.930 (Cost: 194.242) Speed: 6055 wps\n",
      "37.7% Perplexity: 202.344 (Cost: 194.570) Speed: 6057 wps\n",
      "38.4% Perplexity: 202.928 (Cost: 183.906) Speed: 6059 wps\n",
      "39.2% Perplexity: 202.611 (Cost: 183.890) Speed: 6061 wps\n",
      "39.9% Perplexity: 202.395 (Cost: 178.242) Speed: 6061 wps\n",
      "40.7% Perplexity: 203.150 (Cost: 195.558) Speed: 6061 wps\n",
      "41.4% Perplexity: 202.842 (Cost: 173.457) Speed: 6061 wps\n",
      "42.2% Perplexity: 202.629 (Cost: 182.455) Speed: 6061 wps\n",
      "43.0% Perplexity: 201.903 (Cost: 175.838) Speed: 6061 wps\n",
      "43.7% Perplexity: 201.489 (Cost: 188.321) Speed: 6062 wps\n",
      "44.5% Perplexity: 201.885 (Cost: 194.332) Speed: 6064 wps\n",
      "45.2% Perplexity: 202.490 (Cost: 189.679) Speed: 6063 wps\n",
      "46.0% Perplexity: 202.876 (Cost: 187.316) Speed: 6064 wps\n",
      "46.7% Perplexity: 203.146 (Cost: 182.973) Speed: 6063 wps\n",
      "47.5% Perplexity: 203.198 (Cost: 193.482) Speed: 6063 wps\n",
      "48.2% Perplexity: 203.202 (Cost: 181.433) Speed: 6064 wps\n",
      "49.0% Perplexity: 202.813 (Cost: 177.236) Speed: 6065 wps\n",
      "49.7% Perplexity: 202.754 (Cost: 179.789) Speed: 6066 wps\n",
      "50.5% Perplexity: 201.928 (Cost: 173.866) Speed: 6066 wps\n",
      "51.2% Perplexity: 201.497 (Cost: 185.689) Speed: 6066 wps\n",
      "52.0% Perplexity: 201.418 (Cost: 186.592) Speed: 6066 wps\n",
      "52.8% Perplexity: 201.106 (Cost: 186.234) Speed: 6066 wps\n",
      "53.5% Perplexity: 201.120 (Cost: 179.874) Speed: 6067 wps\n",
      "54.3% Perplexity: 200.867 (Cost: 175.470) Speed: 6068 wps\n",
      "55.0% Perplexity: 200.206 (Cost: 174.409) Speed: 6067 wps\n",
      "55.8% Perplexity: 199.907 (Cost: 175.483) Speed: 6067 wps\n",
      "56.5% Perplexity: 199.586 (Cost: 179.226) Speed: 6067 wps\n",
      "57.3% Perplexity: 198.942 (Cost: 181.450) Speed: 6067 wps\n",
      "58.0% Perplexity: 198.584 (Cost: 180.720) Speed: 6067 wps\n",
      "58.8% Perplexity: 198.182 (Cost: 173.088) Speed: 6068 wps\n",
      "59.5% Perplexity: 198.044 (Cost: 183.809) Speed: 6069 wps\n",
      "60.3% Perplexity: 197.927 (Cost: 184.548) Speed: 6069 wps\n",
      "61.0% Perplexity: 198.009 (Cost: 183.169) Speed: 6068 wps\n",
      "61.8% Perplexity: 197.623 (Cost: 187.436) Speed: 6068 wps\n",
      "62.5% Perplexity: 197.301 (Cost: 184.085) Speed: 6068 wps\n",
      "63.3% Perplexity: 197.404 (Cost: 185.190) Speed: 6068 wps\n",
      "64.1% Perplexity: 197.251 (Cost: 181.754) Speed: 6069 wps\n",
      "64.8% Perplexity: 197.017 (Cost: 173.883) Speed: 6069 wps\n",
      "65.6% Perplexity: 196.698 (Cost: 168.374) Speed: 6069 wps\n",
      "66.3% Perplexity: 196.349 (Cost: 173.909) Speed: 6069 wps\n",
      "67.1% Perplexity: 196.609 (Cost: 195.971) Speed: 6069 wps\n",
      "67.8% Perplexity: 196.756 (Cost: 185.438) Speed: 6069 wps\n",
      "68.6% Perplexity: 196.553 (Cost: 189.557) Speed: 6070 wps\n",
      "69.3% Perplexity: 196.592 (Cost: 183.398) Speed: 6070 wps\n",
      "70.1% Perplexity: 196.488 (Cost: 200.113) Speed: 6071 wps\n",
      "70.8% Perplexity: 196.509 (Cost: 179.780) Speed: 6071 wps\n",
      "71.6% Perplexity: 196.557 (Cost: 190.792) Speed: 6070 wps\n",
      "72.3% Perplexity: 196.799 (Cost: 185.226) Speed: 6070 wps\n",
      "73.1% Perplexity: 196.768 (Cost: 188.585) Speed: 6070 wps\n",
      "73.9% Perplexity: 196.810 (Cost: 184.914) Speed: 6070 wps\n",
      "74.6% Perplexity: 196.544 (Cost: 189.728) Speed: 6071 wps\n",
      "75.4% Perplexity: 196.251 (Cost: 181.686) Speed: 6071 wps\n",
      "76.1% Perplexity: 196.360 (Cost: 190.554) Speed: 6071 wps\n",
      "76.9% Perplexity: 196.373 (Cost: 181.592) Speed: 6071 wps\n",
      "77.6% Perplexity: 196.447 (Cost: 182.146) Speed: 6070 wps\n",
      "78.4% Perplexity: 196.066 (Cost: 168.034) Speed: 6071 wps\n",
      "79.1% Perplexity: 195.723 (Cost: 181.182) Speed: 6071 wps\n",
      "79.9% Perplexity: 195.574 (Cost: 172.759) Speed: 6072 wps\n",
      "80.6% Perplexity: 194.973 (Cost: 166.803) Speed: 6072 wps\n",
      "81.4% Perplexity: 194.433 (Cost: 178.228) Speed: 6071 wps\n",
      "82.1% Perplexity: 193.901 (Cost: 182.946) Speed: 6071 wps\n",
      "82.9% Perplexity: 193.632 (Cost: 178.634) Speed: 6071 wps\n",
      "83.6% Perplexity: 193.629 (Cost: 184.644) Speed: 6071 wps\n",
      "84.4% Perplexity: 193.645 (Cost: 183.064) Speed: 6071 wps\n",
      "85.2% Perplexity: 193.537 (Cost: 176.944) Speed: 6072 wps\n",
      "85.9% Perplexity: 193.496 (Cost: 188.254) Speed: 6072 wps\n",
      "86.7% Perplexity: 193.129 (Cost: 163.889) Speed: 6071 wps\n",
      "87.4% Perplexity: 192.984 (Cost: 179.977) Speed: 6071 wps\n",
      "88.2% Perplexity: 192.677 (Cost: 186.507) Speed: 6071 wps\n",
      "88.9% Perplexity: 192.638 (Cost: 171.350) Speed: 6071 wps\n",
      "89.7% Perplexity: 192.205 (Cost: 176.416) Speed: 6071 wps\n",
      "90.4% Perplexity: 191.722 (Cost: 176.373) Speed: 6072 wps\n",
      "91.2% Perplexity: 191.237 (Cost: 174.629) Speed: 6072 wps\n",
      "91.9% Perplexity: 190.885 (Cost: 184.284) Speed: 6072 wps\n",
      "92.7% Perplexity: 190.529 (Cost: 179.644) Speed: 6071 wps\n",
      "93.4% Perplexity: 190.559 (Cost: 183.295) Speed: 6071 wps\n",
      "94.2% Perplexity: 190.276 (Cost: 178.328) Speed: 6072 wps\n",
      "95.0% Perplexity: 190.052 (Cost: 178.542) Speed: 6072 wps\n",
      "95.7% Perplexity: 189.812 (Cost: 174.264) Speed: 6072 wps\n",
      "96.5% Perplexity: 189.623 (Cost: 185.899) Speed: 6071 wps\n",
      "97.2% Perplexity: 189.690 (Cost: 193.356) Speed: 6071 wps\n",
      "98.0% Perplexity: 189.909 (Cost: 185.632) Speed: 6071 wps\n",
      "98.7% Perplexity: 189.876 (Cost: 180.021) Speed: 6071 wps\n",
      "99.5% Perplexity: 190.030 (Cost: 187.396) Speed: 6071 wps\n",
      "Epoch: 3 Training Perplexity: 190.094 (Cost: 5.248)\n",
      "Epoch: 3 Validation Perplexity: 173.922 (Cost: 5.159)\n",
      "Epoch: 4 Learning Rate: 1.000\n",
      "0.0% Perplexity: 259.702 (Cost: 194.584) Speed: 2760 wps\n",
      "0.8% Perplexity: 206.439 (Cost: 186.129) Speed: 5075 wps\n",
      "1.5% Perplexity: 183.787 (Cost: 187.602) Speed: 5495 wps\n",
      "2.3% Perplexity: 182.932 (Cost: 178.843) Speed: 5683 wps\n",
      "3.0% Perplexity: 180.880 (Cost: 186.069) Speed: 5771 wps\n",
      "3.8% Perplexity: 177.520 (Cost: 179.806) Speed: 5825 wps\n",
      "4.5% Perplexity: 177.216 (Cost: 184.864) Speed: 5868 wps\n",
      "5.3% Perplexity: 174.225 (Cost: 180.905) Speed: 5891 wps\n",
      "6.0% Perplexity: 169.974 (Cost: 171.376) Speed: 5921 wps\n",
      "6.8% Perplexity: 166.637 (Cost: 177.950) Speed: 5937 wps\n",
      "7.5% Perplexity: 164.109 (Cost: 171.369) Speed: 5952 wps\n",
      "8.3% Perplexity: 163.682 (Cost: 179.535) Speed: 5964 wps\n",
      "9.0% Perplexity: 162.682 (Cost: 173.730) Speed: 5969 wps\n",
      "9.8% Perplexity: 162.813 (Cost: 175.977) Speed: 5982 wps\n",
      "10.6% Perplexity: 163.841 (Cost: 179.034) Speed: 5988 wps\n",
      "11.3% Perplexity: 165.643 (Cost: 188.607) Speed: 5994 wps\n",
      "12.1% Perplexity: 166.848 (Cost: 180.839) Speed: 5999 wps\n",
      "12.8% Perplexity: 168.866 (Cost: 187.773) Speed: 6005 wps\n",
      "13.6% Perplexity: 170.007 (Cost: 184.024) Speed: 6009 wps\n",
      "14.3% Perplexity: 171.022 (Cost: 185.584) Speed: 6013 wps\n",
      "15.1% Perplexity: 171.837 (Cost: 187.899) Speed: 6016 wps\n",
      "15.8% Perplexity: 172.901 (Cost: 179.476) Speed: 6020 wps\n",
      "16.6% Perplexity: 173.254 (Cost: 183.979) Speed: 6022 wps\n",
      "17.3% Perplexity: 173.966 (Cost: 177.017) Speed: 6024 wps\n",
      "18.1% Perplexity: 173.651 (Cost: 175.375) Speed: 6027 wps\n",
      "18.8% Perplexity: 174.270 (Cost: 182.751) Speed: 6032 wps\n",
      "19.6% Perplexity: 174.810 (Cost: 183.037) Speed: 6032 wps\n",
      "20.3% Perplexity: 175.471 (Cost: 187.367) Speed: 6034 wps\n",
      "21.1% Perplexity: 175.663 (Cost: 174.511) Speed: 6037 wps\n",
      "21.9% Perplexity: 175.883 (Cost: 182.256) Speed: 6039 wps\n",
      "22.6% Perplexity: 174.939 (Cost: 172.321) Speed: 6040 wps\n",
      "23.4% Perplexity: 173.406 (Cost: 170.329) Speed: 6041 wps\n",
      "24.1% Perplexity: 171.933 (Cost: 163.633) Speed: 6044 wps\n",
      "24.9% Perplexity: 171.406 (Cost: 181.224) Speed: 6044 wps\n",
      "25.6% Perplexity: 172.477 (Cost: 194.053) Speed: 6044 wps\n",
      "26.4% Perplexity: 173.679 (Cost: 181.654) Speed: 6046 wps\n",
      "27.1% Perplexity: 173.525 (Cost: 174.545) Speed: 6048 wps\n",
      "27.9% Perplexity: 172.920 (Cost: 191.142) Speed: 6050 wps\n",
      "28.6% Perplexity: 172.588 (Cost: 165.623) Speed: 6051 wps\n",
      "29.4% Perplexity: 172.798 (Cost: 188.907) Speed: 6051 wps\n",
      "30.1% Perplexity: 172.959 (Cost: 179.698) Speed: 6052 wps\n",
      "30.9% Perplexity: 172.234 (Cost: 176.574) Speed: 6053 wps\n",
      "31.7% Perplexity: 172.211 (Cost: 184.755) Speed: 6052 wps\n",
      "32.4% Perplexity: 172.006 (Cost: 176.448) Speed: 6052 wps\n",
      "33.2% Perplexity: 172.120 (Cost: 171.151) Speed: 6054 wps\n",
      "33.9% Perplexity: 172.105 (Cost: 180.272) Speed: 6054 wps\n",
      "34.7% Perplexity: 172.115 (Cost: 186.199) Speed: 6054 wps\n",
      "35.4% Perplexity: 172.075 (Cost: 182.213) Speed: 6054 wps\n",
      "36.2% Perplexity: 171.675 (Cost: 185.890) Speed: 6054 wps\n",
      "36.9% Perplexity: 171.608 (Cost: 189.954) Speed: 6054 wps\n",
      "37.7% Perplexity: 171.947 (Cost: 189.103) Speed: 6055 wps\n",
      "38.4% Perplexity: 172.494 (Cost: 179.823) Speed: 6055 wps\n",
      "39.2% Perplexity: 172.358 (Cost: 178.413) Speed: 6056 wps\n",
      "39.9% Perplexity: 172.368 (Cost: 173.670) Speed: 6057 wps\n",
      "40.7% Perplexity: 173.068 (Cost: 190.607) Speed: 6056 wps\n",
      "41.4% Perplexity: 172.758 (Cost: 167.715) Speed: 6057 wps\n",
      "42.2% Perplexity: 172.612 (Cost: 179.620) Speed: 6057 wps\n",
      "43.0% Perplexity: 172.026 (Cost: 170.410) Speed: 6058 wps\n",
      "43.7% Perplexity: 171.673 (Cost: 183.495) Speed: 6058 wps\n",
      "44.5% Perplexity: 172.043 (Cost: 190.356) Speed: 6058 wps\n",
      "45.2% Perplexity: 172.642 (Cost: 183.086) Speed: 6058 wps\n",
      "46.0% Perplexity: 173.003 (Cost: 180.607) Speed: 6059 wps\n",
      "46.7% Perplexity: 173.329 (Cost: 175.049) Speed: 6059 wps\n",
      "47.5% Perplexity: 173.403 (Cost: 188.474) Speed: 6059 wps\n",
      "48.2% Perplexity: 173.491 (Cost: 179.103) Speed: 6059 wps\n",
      "49.0% Perplexity: 173.344 (Cost: 174.130) Speed: 6059 wps\n",
      "49.7% Perplexity: 173.367 (Cost: 177.573) Speed: 6060 wps\n",
      "50.5% Perplexity: 172.677 (Cost: 167.701) Speed: 6060 wps\n",
      "51.2% Perplexity: 172.397 (Cost: 178.670) Speed: 6060 wps\n",
      "52.0% Perplexity: 172.455 (Cost: 184.254) Speed: 6060 wps\n",
      "52.8% Perplexity: 172.264 (Cost: 182.914) Speed: 6061 wps\n",
      "53.5% Perplexity: 172.263 (Cost: 174.895) Speed: 6060 wps\n",
      "54.3% Perplexity: 172.161 (Cost: 171.703) Speed: 6060 wps\n",
      "55.0% Perplexity: 171.640 (Cost: 169.832) Speed: 6060 wps\n",
      "55.8% Perplexity: 171.378 (Cost: 169.560) Speed: 6061 wps\n",
      "56.5% Perplexity: 171.030 (Cost: 168.987) Speed: 6061 wps\n",
      "57.3% Perplexity: 170.494 (Cost: 172.400) Speed: 6061 wps\n",
      "58.0% Perplexity: 170.270 (Cost: 174.822) Speed: 6061 wps\n",
      "58.8% Perplexity: 169.957 (Cost: 165.906) Speed: 6061 wps\n",
      "59.5% Perplexity: 169.946 (Cost: 180.267) Speed: 6061 wps\n",
      "60.3% Perplexity: 169.901 (Cost: 181.194) Speed: 6061 wps\n",
      "61.0% Perplexity: 170.045 (Cost: 180.014) Speed: 6061 wps\n",
      "61.8% Perplexity: 169.802 (Cost: 183.644) Speed: 6061 wps\n",
      "62.5% Perplexity: 169.547 (Cost: 180.734) Speed: 6061 wps\n",
      "63.3% Perplexity: 169.649 (Cost: 181.532) Speed: 6062 wps\n",
      "64.1% Perplexity: 169.578 (Cost: 176.590) Speed: 6062 wps\n",
      "64.8% Perplexity: 169.435 (Cost: 169.925) Speed: 6062 wps\n",
      "65.6% Perplexity: 169.228 (Cost: 163.824) Speed: 6062 wps\n",
      "66.3% Perplexity: 169.003 (Cost: 170.213) Speed: 6062 wps\n",
      "67.1% Perplexity: 169.219 (Cost: 189.770) Speed: 6063 wps\n",
      "67.8% Perplexity: 169.356 (Cost: 177.812) Speed: 6062 wps\n",
      "68.6% Perplexity: 169.175 (Cost: 185.437) Speed: 6062 wps\n",
      "69.3% Perplexity: 169.223 (Cost: 179.785) Speed: 6062 wps\n",
      "70.1% Perplexity: 169.191 (Cost: 195.046) Speed: 6062 wps\n",
      "70.8% Perplexity: 169.305 (Cost: 178.758) Speed: 6063 wps\n",
      "71.6% Perplexity: 169.349 (Cost: 185.837) Speed: 6063 wps\n",
      "72.3% Perplexity: 169.615 (Cost: 180.927) Speed: 6063 wps\n",
      "73.1% Perplexity: 169.621 (Cost: 183.647) Speed: 6064 wps\n",
      "73.9% Perplexity: 169.723 (Cost: 182.676) Speed: 6063 wps\n",
      "74.6% Perplexity: 169.563 (Cost: 184.904) Speed: 6063 wps\n",
      "75.4% Perplexity: 169.332 (Cost: 178.863) Speed: 6063 wps\n",
      "76.1% Perplexity: 169.455 (Cost: 185.395) Speed: 6063 wps\n",
      "76.9% Perplexity: 169.492 (Cost: 177.348) Speed: 6064 wps\n",
      "77.6% Perplexity: 169.595 (Cost: 179.419) Speed: 6064 wps\n",
      "78.4% Perplexity: 169.268 (Cost: 163.797) Speed: 6064 wps\n",
      "79.1% Perplexity: 169.033 (Cost: 179.220) Speed: 6064 wps\n",
      "79.9% Perplexity: 168.953 (Cost: 167.524) Speed: 6064 wps\n",
      "80.6% Perplexity: 168.483 (Cost: 161.900) Speed: 6064 wps\n",
      "81.4% Perplexity: 168.050 (Cost: 174.109) Speed: 6064 wps\n",
      "82.1% Perplexity: 167.604 (Cost: 177.606) Speed: 6063 wps\n",
      "82.9% Perplexity: 167.415 (Cost: 175.088) Speed: 6063 wps\n",
      "83.6% Perplexity: 167.438 (Cost: 182.040) Speed: 6063 wps\n",
      "84.4% Perplexity: 167.468 (Cost: 179.562) Speed: 6063 wps\n",
      "85.2% Perplexity: 167.430 (Cost: 172.198) Speed: 6063 wps\n",
      "85.9% Perplexity: 167.426 (Cost: 181.688) Speed: 6063 wps\n",
      "86.7% Perplexity: 167.099 (Cost: 158.719) Speed: 6063 wps\n",
      "87.4% Perplexity: 167.020 (Cost: 171.466) Speed: 6063 wps\n",
      "88.2% Perplexity: 166.830 (Cost: 181.654) Speed: 6063 wps\n",
      "88.9% Perplexity: 166.839 (Cost: 169.833) Speed: 6063 wps\n",
      "89.7% Perplexity: 166.521 (Cost: 174.326) Speed: 6063 wps\n",
      "90.4% Perplexity: 166.156 (Cost: 172.285) Speed: 6063 wps\n",
      "91.2% Perplexity: 165.756 (Cost: 167.888) Speed: 6063 wps\n",
      "91.9% Perplexity: 165.514 (Cost: 181.335) Speed: 6063 wps\n",
      "92.7% Perplexity: 165.231 (Cost: 177.230) Speed: 6063 wps\n",
      "93.4% Perplexity: 165.291 (Cost: 179.783) Speed: 6063 wps\n",
      "94.2% Perplexity: 165.118 (Cost: 176.490) Speed: 6063 wps\n",
      "95.0% Perplexity: 164.982 (Cost: 173.738) Speed: 6063 wps\n",
      "95.7% Perplexity: 164.831 (Cost: 172.950) Speed: 6063 wps\n",
      "96.5% Perplexity: 164.707 (Cost: 183.011) Speed: 6063 wps\n",
      "97.2% Perplexity: 164.748 (Cost: 187.729) Speed: 6063 wps\n",
      "98.0% Perplexity: 164.932 (Cost: 182.583) Speed: 6063 wps\n",
      "98.7% Perplexity: 164.957 (Cost: 175.703) Speed: 6064 wps\n",
      "99.5% Perplexity: 165.130 (Cost: 184.307) Speed: 6063 wps\n",
      "Epoch: 4 Training Perplexity: 165.204 (Cost: 5.107)\n",
      "Epoch: 4 Validation Perplexity: 154.025 (Cost: 5.037)\n",
      "Epoch: 5 Learning Rate: 1.000\n",
      "0.0% Perplexity: 224.722 (Cost: 189.520) Speed: 2846 wps\n",
      "0.8% Perplexity: 185.118 (Cost: 183.554) Speed: 5263 wps\n",
      "1.5% Perplexity: 163.307 (Cost: 183.933) Speed: 5628 wps\n",
      "2.3% Perplexity: 160.339 (Cost: 172.206) Speed: 5773 wps\n",
      "3.0% Perplexity: 159.334 (Cost: 182.002) Speed: 5830 wps\n",
      "3.8% Perplexity: 156.622 (Cost: 175.148) Speed: 5874 wps\n",
      "4.5% Perplexity: 156.239 (Cost: 182.959) Speed: 5902 wps\n",
      "5.3% Perplexity: 154.161 (Cost: 175.259) Speed: 5929 wps\n",
      "6.0% Perplexity: 150.574 (Cost: 169.801) Speed: 5953 wps\n",
      "6.8% Perplexity: 147.716 (Cost: 171.955) Speed: 5966 wps\n",
      "7.5% Perplexity: 145.339 (Cost: 167.181) Speed: 5980 wps\n",
      "8.3% Perplexity: 144.900 (Cost: 175.915) Speed: 5985 wps\n",
      "9.0% Perplexity: 144.501 (Cost: 170.181) Speed: 5993 wps\n",
      "9.8% Perplexity: 144.799 (Cost: 172.758) Speed: 5998 wps\n",
      "10.6% Perplexity: 145.768 (Cost: 175.056) Speed: 6003 wps\n",
      "11.3% Perplexity: 147.463 (Cost: 183.759) Speed: 6011 wps\n",
      "12.1% Perplexity: 148.667 (Cost: 177.936) Speed: 6017 wps\n",
      "12.8% Perplexity: 150.749 (Cost: 182.623) Speed: 6020 wps\n",
      "13.6% Perplexity: 151.998 (Cost: 181.995) Speed: 6024 wps\n",
      "14.3% Perplexity: 152.776 (Cost: 176.974) Speed: 6025 wps\n",
      "15.1% Perplexity: 153.784 (Cost: 183.752) Speed: 6024 wps\n",
      "15.8% Perplexity: 154.861 (Cost: 175.655) Speed: 6028 wps\n",
      "16.6% Perplexity: 155.012 (Cost: 181.960) Speed: 6032 wps\n",
      "17.3% Perplexity: 155.744 (Cost: 174.469) Speed: 6038 wps\n",
      "18.1% Perplexity: 155.525 (Cost: 171.398) Speed: 6044 wps\n",
      "18.8% Perplexity: 156.170 (Cost: 179.136) Speed: 6045 wps\n",
      "19.6% Perplexity: 156.834 (Cost: 178.942) Speed: 6047 wps\n",
      "20.3% Perplexity: 157.532 (Cost: 186.789) Speed: 6047 wps\n",
      "21.1% Perplexity: 157.712 (Cost: 168.912) Speed: 6046 wps\n",
      "21.9% Perplexity: 157.877 (Cost: 179.859) Speed: 6045 wps\n",
      "22.6% Perplexity: 157.018 (Cost: 168.355) Speed: 6046 wps\n",
      "23.4% Perplexity: 155.800 (Cost: 168.290) Speed: 6049 wps\n",
      "24.1% Perplexity: 154.468 (Cost: 159.057) Speed: 6052 wps\n",
      "24.9% Perplexity: 154.043 (Cost: 177.846) Speed: 6053 wps\n",
      "25.6% Perplexity: 154.937 (Cost: 192.154) Speed: 6054 wps\n",
      "26.4% Perplexity: 155.989 (Cost: 173.935) Speed: 6055 wps\n",
      "27.1% Perplexity: 155.908 (Cost: 170.169) Speed: 6056 wps\n",
      "27.9% Perplexity: 155.391 (Cost: 186.109) Speed: 6057 wps\n",
      "28.6% Perplexity: 155.075 (Cost: 157.946) Speed: 6058 wps\n",
      "29.4% Perplexity: 155.273 (Cost: 186.220) Speed: 6060 wps\n",
      "30.1% Perplexity: 155.419 (Cost: 175.641) Speed: 6062 wps\n",
      "30.9% Perplexity: 154.729 (Cost: 171.895) Speed: 6064 wps\n",
      "31.7% Perplexity: 154.681 (Cost: 180.340) Speed: 6063 wps\n",
      "32.4% Perplexity: 154.496 (Cost: 171.604) Speed: 6064 wps\n",
      "33.2% Perplexity: 154.621 (Cost: 166.562) Speed: 6064 wps\n",
      "33.9% Perplexity: 154.629 (Cost: 176.446) Speed: 6065 wps\n",
      "34.7% Perplexity: 154.586 (Cost: 182.387) Speed: 6065 wps\n",
      "35.4% Perplexity: 154.602 (Cost: 179.398) Speed: 6065 wps\n",
      "36.2% Perplexity: 154.296 (Cost: 182.771) Speed: 6067 wps\n",
      "36.9% Perplexity: 154.237 (Cost: 185.748) Speed: 6068 wps\n",
      "37.7% Perplexity: 154.566 (Cost: 184.859) Speed: 6070 wps\n",
      "38.4% Perplexity: 155.098 (Cost: 174.022) Speed: 6070 wps\n",
      "39.2% Perplexity: 155.035 (Cost: 174.966) Speed: 6070 wps\n",
      "39.9% Perplexity: 155.114 (Cost: 170.574) Speed: 6070 wps\n",
      "40.7% Perplexity: 155.716 (Cost: 187.712) Speed: 6071 wps\n",
      "41.4% Perplexity: 155.438 (Cost: 164.389) Speed: 6072 wps\n",
      "42.2% Perplexity: 155.380 (Cost: 175.829) Speed: 6072 wps\n",
      "43.0% Perplexity: 154.866 (Cost: 164.303) Speed: 6074 wps\n",
      "43.7% Perplexity: 154.637 (Cost: 180.342) Speed: 6074 wps\n",
      "44.5% Perplexity: 154.966 (Cost: 185.593) Speed: 6076 wps\n",
      "45.2% Perplexity: 155.574 (Cost: 182.345) Speed: 6076 wps\n",
      "46.0% Perplexity: 155.929 (Cost: 176.442) Speed: 6075 wps\n",
      "46.7% Perplexity: 156.227 (Cost: 173.012) Speed: 6076 wps\n",
      "47.5% Perplexity: 156.318 (Cost: 185.192) Speed: 6077 wps\n",
      "48.2% Perplexity: 156.453 (Cost: 177.087) Speed: 6078 wps\n",
      "49.0% Perplexity: 156.301 (Cost: 170.820) Speed: 6079 wps\n",
      "49.7% Perplexity: 156.306 (Cost: 171.570) Speed: 6079 wps\n",
      "50.5% Perplexity: 155.755 (Cost: 167.469) Speed: 6079 wps\n",
      "51.2% Perplexity: 155.539 (Cost: 176.367) Speed: 6080 wps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cbeb0a44f059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# run training pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtrain_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %i Training Perplexity: %.3f (Cost: %.3f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mtrain_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cbeb0a44f059>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(sess, model, data, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         })\n\u001b[1;32m     19\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    # initial RNN state\n",
    "#     state = model.initial_state.eval()\n",
    "    state = sess.run(model.initial_state)\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        perplexity = np.exp(costs / iters)\n",
    "\n",
    "        if verbose and step % 10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"%.1f%% Perplexity: %.3f (Cost: %.3f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "\n",
    "    return (costs / iters), perplexity\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    learning_rate = 1.0\n",
    "    lr_decay = 0.8\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch\n",
    "\n",
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "cell_types = {\n",
    "    'lstm': VanillaLSTMCell,\n",
    "    'vanillaRNN': VanillaRNNCell,\n",
    "    'gru': GruCell,\n",
    "    'mut1': MUT1,\n",
    "    'mut2': MUT2,\n",
    "    'mut3': MUT3,\n",
    "    'nig': NIGLSTMCell,\n",
    "    'nfg': NFGLSTMCell,\n",
    "    'nog': NOGLSTMCell,\n",
    "    'niaf': NIAFLSTMCell,\n",
    "    'noaf': NOAFLSTMCell,\n",
    "    'np': NPLSTMCell,\n",
    "    'cifg': CIFGLSTMCell,\n",
    "    'fgr': FGRLSTMCell\n",
    "}\n",
    "\n",
    "model_name = \"mut3\"\n",
    "\n",
    "model_path = './mut1_model/'\n",
    "checkpoint_path = './'+model_name\n",
    "summary_path = './mut1_model/'\n",
    "CellType = cell_types[model_name]\n",
    "\n",
    "# CellType = rnn_cell.GRUCell\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "\n",
    "    # we create a separate model for validation and testing to alter the batch size and time steps\n",
    "    # reuse=True reuses variables from the previously defined `train_model`\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Epoch: %d Learning Rate: %.3f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.3f (Cost: %.3f)\" % (i + 1, train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.3f (Cost: %.3f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint')\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.3f (Cost: %.3f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you want to see the data format that is going into the model, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = enumerate(ptb_reader.ptb_iterator(train_data, batch_size=20, num_steps=35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step, (x, y) = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
